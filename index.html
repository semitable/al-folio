<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Filippos  Christianos


</title>
<meta name="description" content="Personal website of Filippos Christianos.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>





<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=Ffn3mvdBwKS"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: 'Ffn3mvdBwKS' });
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%66%69%6C%69%70%70%6F%73.%63%68%72%69%73%74%69%61%6E%6F%73@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=q09VRMkAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/semitable" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/fchristianos" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/f_christianos" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>










        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Filippos</span>  Christianos
    </h1>
     <p class="desc"><a href="https://www.ed.ac.uk/informatics">University of Edinburgh, School of Informatics</a>.</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am a PhD student in the <a href="https://www.edinburgh-robotics.org/">CDT for Robotics and Autonomous Agents</a>, advised by <a href="https://agents.inf.ed.ac.uk/stefano-albrecht/">Stefano Albrecht</a> (<a href="https://www.ed.ac.uk/informatics">University of Edinburgh</a>) and a member of the <a href="https://agents.inf.ed.ac.uk/">Autonomous Agents Research Group</a>.</p>

<p>My PhD research is in the area of Multi-Agent Deep Reinforcement Learning. In particular, I study how multiple agents can efficiently explore and learn in environments with sparse rewards.</p>

<p>I am the author and maintainer of the <a href="https://www.github.com/semitable/robotic-warehouse">Multi-Robot Warehouse</a> environment for multi-agent RL research. I also developed and maintain the Python version of <a href="https://www.github.com/semitable/lb-foraging">Level-based Foraging</a>. Our group has been using both environments to develop new and exciting algorithms for MARL. I am the first author of two such algorithms: Shared Experience Actor-Critic (SEAC), and Selective Parameter Sharing (SePS) that have been published in NeurIPS (2020) and ICML (2021) respectively.</p>

<p>Keywords: Machine Learning, Deep Reinforcement Learning (RL), Multi-agent Systems, Exploration in RL.</p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">May 10, 2021</th>
          <td>
            
              <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"><img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20"> Two new papers accepted at ICML 2021: <a href="https://arxiv.org/abs/2102.07475">Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing</a> and <a href="https://arxiv.org/abs/2006.10412">Towards Open Ad Hoc Teamwork Using Graph-based Policy Learning</a>.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 20, 2020</th>
          <td>
            
              <img class="emoji" title=":robot:" alt=":robot:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f916.png" height="20" width="20"> My blog post on two new environments for MARL has just been <a href="https://agents.inf.ed.ac.uk/blog/new-environments-algorithm-multiagent-rl/">posted</a> in our groups webpage.

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 8, 2020</th>
          <td>
            
              <img class="emoji" title=":newspaper_roll:" alt=":newspaper_roll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f5de.png" height="20" width="20">    Our paper, <a href="https://arxiv.org/abs/2006.07169">Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning</a>, has been accepted and published in Neural Information Processing Systems (NeurIPS 2020).

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICML</abbr>
    
  
  </div>

  <div id="christianosScalingMultiAgentReinforcement2021" class="col-sm-8">
    
      <div class="title">Scaling Multi-Agent Reinforcement Learning with Selective Parameter Sharing</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Christianos Filippos</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Papoudakis Georgios,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Rahman Arrasy,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Albrecht Stefano
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the 38th International Conference on Machine Learning</em>, 2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      <a href="http://proceedings.mlr.press/v139/christianos21a.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Paper</a>
    
    
    
    
    
      <a href="https://github.com/semitable/seps" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/seps_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/pdf/seps_slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Sharing parameters in multi-agent deep reinforcement learning has played an essential role in allowing algorithms to scale to a large number of agents. Parameter sharing between agents significantly decreases the number of trainable parameters, shortening training times to tractable levels, and has been linked to more efficient learning. However, having all agents share the same parameters can also have a detrimental effect on learning. We demonstrate the impact of parameter sharing methods on training speed and converged returns, establishing that when applied indiscriminately, their effectiveness is highly dependent on the environment. We propose a novel method to automatically identify agents which may benefit from sharing parameters by partitioning them based on their abilities and goals. Our approach combines the increased sample efficiency of parameter sharing with the representational capacity of multiple independent networks to reduce training time and increase final returns.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ALA</abbr>
    
  
  </div>

  <div id="papoudakis2021benchmarking" class="col-sm-8">
    
      <div class="title">Comparative Evaluation of Cooperative Multi-Agent Deep Reinforcement Learning Algorithms</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Papoudakis Georgios *,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Christianos Filippos *</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  SchÃ¤fer Lukas,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Albrecht Stefano V.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In AAMAS Workshop on Adaptive and Learning Agents Workshop</em>, 2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
      <a href="http://arxiv.org/abs/2006.07869" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
    
    
    
    <a href="https://openreview.net/forum?id=cIrPX-Sn5n" class="btn btn-sm z-depth-0" role="button" target="_blank">OpenReview</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we evaluate and compare three different classes of MARL algorithms (independent learners, centralised training with decentralised execution, and value decomposition) in a diverse range of multi-agent learning tasks. Our results show that (1) algorithm performance depends strongly on environment properties and no algorithm learns efficiently across all learning tasks; (2) independent learners often achieve equal or better performance than more complex algorithms; (3) tested algorithms struggle to solve multi-agent tasks with sparse rewards. We report detailed empirical data, including a reliability analysis, and provide insights into the limitations of the tested algorithms.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">NeurIPS</abbr>
    
  
  </div>

  <div id="christianosSharedExperienceActorCritic2020" class="col-sm-8">
    
      <div class="title">Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Christianos Filippos</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  SchÃ¤fer Lukas,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Albrecht Stefano
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Advances in Neural Information Processing Systems</em>, 2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
      <a href="https://proceedings.neurips.cc/paper/2020/hash/7967cc8e3ab559e68cc944c44b1cf3e8-Abstract.html" class="btn btn-sm z-depth-0" role="button" target="_blank">Paper</a>
    
    
    
    
    
      <a href="https://github.com/semitable/seac" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
      
      <a href="/assets/pdf/seac_poster.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
      
    
    
      
      <a href="/assets/pdf/seac_slides.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Slides</a>
      
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called shared Experience Actor-Critic(SEAC), applies experience sharing in an actor-critic framework by combining the gradients of different agents. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms several baselines and state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECAI</abbr>
    
  
  </div>

  <div id="christianosEmployingHypergraphsEfficient2016" class="col-sm-8">
    
      <div class="title">Employing Hypergraphs for Efficient Coalition Formation with Application to the V2G Problem</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Christianos Filippos</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Chalkiadakis Georgios
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proceedings of the Twenty-Second European Conference on Artificial Intelligence</em>, 2016
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This paper proposes, for the first time in the literature, the use of hypergraphs for the efficient formation of effective coalitions. We put forward several formation methods that build on existing hypergraph algorithms, and exploit hypergraph structure to identify agents with desirable characteristics. Our approach allows the near-instantaneous formation of high quality coalitions, while adhering to multiple stated requirements regarding coalition quality. Moreover, our methods are shown to scale to dozens of thousands of agents within fractions of a second; with one of them scaling to even millions of agents within seconds. We apply our approach to the problem of forming coalitions to provide (electric) vehicle-to-grid (V2G) services. Ours is the first approach able to deal with large-scale, realtime coalition formation for the V2G problem, while taking multiple criteria into account for creating electric vehicle coalitions.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%66%69%6C%69%70%70%6F%73.%63%68%72%69%73%74%69%61%6E%6F%73@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=q09VRMkAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/semitable" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/fchristianos" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/f_christianos" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>










      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    Â© Copyright 2021 Filippos  Christianos.
    
    
    
    Last updated: July 14, 2021.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
